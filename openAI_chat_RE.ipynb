{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jf/miniconda3/envs/data_challenge_llm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "import warnings\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Append the submodules path to the local libs directory\n",
    "repo_dir = Path().resolve()\n",
    "sys.path.append(str(repo_dir / 'libs'))\n",
    "\n",
    "# Ensure the symlink exists (assuming setup_symlink.py has been executed)\n",
    "symlink_path = repo_dir / 'libs' / 'NLP_on_multilingual_coin_datasets'\n",
    "if not symlink_path.exists():\n",
    "    print(f\"Error: Symlink {symlink_path} does not exist. Run setup_symlink.py first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import the custom modules after ensuring symlink is in place\n",
    "from NLP_on_multilingual_coin_datasets.cnt.io import Database_Connection\n",
    "from modules.loading_preprocessed_designs import PreprocessingConfig, LoadingPreprocessedDesigns\n",
    "from modules import scripts, prompts\n",
    "\n",
    "# Set up pandas display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Access the OpenAI API key from environment variables\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "prep_cfg = PreprocessingConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing config varialbes else default values will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define filenames and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = prep_cfg.json_path\n",
    "enhanced_json_filename = \"enhanced_objects.json\"\n",
    "sop_json_filename = \"subject_object_pairs.json\"\n",
    "pred_json_filename = \"subject_predicate_object_triples.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define database connection parameters or set them as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "database = prep_cfg.database\n",
    "\n",
    "connection_string = f\"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}:{db_port}/{database}\"\n",
    "dc = Database_Connection(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load or preprocess data\n",
    "- Check for the preprocessed designs.csv file in the `data` directory. \n",
    "    - If it does not exist, get the data from the database and preprocess it.\n",
    "    - Else load the data from the file.\n",
    "- Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 20:44:08,767 - INFO - Checking if file data/source/lists/csv/annotated_designs.csv exists.\n",
      "2024-09-02 20:44:08,769 - INFO - File does not exist. Loading from database and running preprocessing.\n",
      "2024-09-02 20:44:08,770 - INFO - Starting preprocessing of designs.\n",
      "2024-09-02 20:44:09,046 - ERROR - Error loading entities: 'NoneType' object is not subscriptable\n",
      "2024-09-02 20:44:09,047 - ERROR - Error during preprocessing: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m lpd \u001b[38;5;241m=\u001b[39m LoadingPreprocessedDesigns(dc, prep_cfg)\n\u001b[0;32m----> 2\u001b[0m df_designs \u001b[38;5;241m=\u001b[39m \u001b[43mlpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_designs_csv_or_process_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/_lokal/DataChallenge_LLM_REPipeline/modules/loading_preprocessed_designs.py:65\u001b[0m, in \u001b[0;36mLoadingPreprocessedDesigns.load_designs_csv_or_process_database\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile does not exist. Loading from database and running preprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m     df_designs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_designs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_cfg\u001b[38;5;241m.\u001b[39mcsv_path)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     df_designs\u001b[38;5;241m.\u001b[39mto_csv(csv_filepath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/_lokal/DataChallenge_LLM_REPipeline/modules/loading_preprocessed_designs.py:80\u001b[0m, in \u001b[0;36mLoadingPreprocessedDesigns.preprocess_designs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     df_designs_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdc\u001b[38;5;241m.\u001b[39mload_designs_from_db(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp_training_designs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     77\u001b[0m                                                   [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_col, \n\u001b[1;32m     78\u001b[0m                                                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_col])\n\u001b[0;32m---> 80\u001b[0m     entities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     annotated_designs \u001b[38;5;241m=\u001b[39m annotate_designs(entities, df_designs_raw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid_col, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesign_col)\n\u001b[1;32m     83\u001b[0m     annotated_designs \u001b[38;5;241m=\u001b[39m annotated_designs[annotated_designs\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/_lokal/DataChallenge_LLM_REPipeline/modules/loading_preprocessed_designs.py:128\u001b[0m, in \u001b[0;36mLoadingPreprocessedDesigns.load_entities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERSON\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOBJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANIMAL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLANT\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    127\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdc\u001b[38;5;241m.\u001b[39mload_entities_from_db_v2(\n\u001b[0;32m--> 128\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatabase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nlp_list_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity_type, add_columns, [\u001b[43madd_columns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m             logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m entities: Received None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "lpd = LoadingPreprocessedDesigns(dc, prep_cfg)\n",
    "df_designs = lpd.load_designs_csv_or_process_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataframe \n",
    "\n",
    "- create copy\n",
    "- filter the columns id, design_en and annotations\n",
    "    - respresenting only the preprocessed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_designs_0 = df_designs.copy()\n",
    "df_designs = df_designs[[\"id\", \"design_en\", \"annotations\"]]\n",
    "df_designs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create strings from annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_designs[\"list_of_strings\"] = df_designs.apply(scripts.generate_list_of_strings, axis=1)\n",
    "df_designs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load RE examples from prepared JSON file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a subset of the data to be used for the testing of the implementation.\n",
    "- 22332 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0 \n",
    "stop = 3275\n",
    "\n",
    "df_designs_source = df_designs.iloc[start:stop].copy()\n",
    "df_designs_source.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Check for More Possible Subjects or Objects\n",
    "- **Input:** Design description and list of strings (entities).\n",
    "- **Output:** Identified subjects and objects categorized as PERSON, OBJECT, ANIMAL, PLANT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_designs_filtered = scripts.filter_source_dataframe(df_designs_source, json_dir, enhanced_json_filename)\n",
    "df_designs_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_start = 0\n",
    "batch_stop = 1\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_enhance = prompts.enhance_objects_in_designs(df_designs_filtered, batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_enhance, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_enhanced = scripts.process_prompts(prompts_enhance, client, batch_start, batch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_enhanced = pd.DataFrame(responses_list_enhanced, index=None)\n",
    "df_responses_enhanced[\"design_id\"] = df_responses_enhanced[\"design_id\"].astype(int)\n",
    "\n",
    "df_enhanced_merged = df_responses_enhanced.merge(df_designs_filtered[['id', 'design_en', 'list_of_strings']], \n",
    "                                       left_on='design_id', right_on='id', how='left'\n",
    "                                       ).drop(columns='id')\n",
    "\n",
    "df_enhanced_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1 Validate and Classify enhanced entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_validate_enhanced = prompts.validate_overall_objects_in_designs(df_enhanced_merged, batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_validate_enhanced, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_validated_enh = scripts.process_prompts(prompts_validate_enhanced, client, batch_start, batch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_val_enhanced = pd.DataFrame(responses_list_validated_enh, index=None)\n",
    "df_enhanced_validated = df_responses_val_enhanced.merge(df_enhanced_merged, on=['design_id'], how='left')\n",
    "\n",
    "df_enhanced_validated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['design_id', 'design_en', 'new_list_of_strings', \n",
    "           'completeness', 'relevance', 'correctness', 'comment_enh', 'list_of_strings']\n",
    "scripts.update_json_with_merged_df(df_enhanced_validated, columns, json_dir, enhanced_json_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Identify Subject-Object Pairs\n",
    "- **Input:** Design description and categorized entities.\n",
    "- **Output:** List of subject-object pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced = pd.read_json(Path(json_dir) / enhanced_json_filename)\n",
    "df_enhanced.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all unique values for completeness, relevance, correctness\n",
    "print(df_enhanced['completeness'].value_counts(),\n",
    "df_enhanced['relevance'].value_counts(),\n",
    "df_enhanced['correctness'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced_filtered = scripts.filter_enhanced_designs(df_enhanced, json_dir, sop_json_filename)\n",
    "df_enhanced_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"batch_start: {batch_start}, batch_stop: {batch_stop}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "batch_size = 16\n",
    "batch_start = 0\n",
    "batch_stop = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO where is id 67 ????\n",
    "prompts_sop = prompts.find_subject_object_pairs_prompts(df_enhanced_filtered, batch_size=batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_sop, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_sop = scripts.process_prompts(prompts_sop, client, batch_start, batch_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_sop = pd.DataFrame(responses_list_sop, index=None)\n",
    "df_responses_sop[\"design_id\"] = df_responses_sop[\"design_id\"].astype(int)\n",
    "# print(df_responses_sop.info())\n",
    "# print(df_designs_filtered.info())\n",
    "df_sop_merged = df_responses_sop.merge(df_enhanced_filtered, on=['design_id'], how='left')\n",
    "# merged_df = merged_df.drop(columns='id')\n",
    "df_sop_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sop_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 Validate and Classify Object Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "length_sop_merged = len(df_sop_merged)\n",
    "print(length_sop_merged)\n",
    "num_batches = (length_sop_merged + batch_size - 1) // batch_size\n",
    "\n",
    "print(num_batches)\n",
    "batch_stop = num_batches\n",
    "\n",
    "prompts_validate_sop = prompts.validate_subject_object_pairs(df_sop_merged, batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_validate_sop, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_validated_sop = scripts.process_prompts(prompts_validate_sop, client, batch_start, batch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_val_sop = pd.DataFrame(responses_list_validated_sop, index=None)\n",
    "df_sop_validated = df_responses_val_sop.merge(df_sop_merged, on=['design_id', 's_o_id'], how='left')\n",
    "\n",
    "df_sop_validated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['design_id', 's_o_id', 's', 'subject_class', 'o', 'object_class', \n",
    "           'validity_sop', 'comment_sop', 'design_en', 'new_list_of_strings', \n",
    "           'completeness', 'relevance', 'correctness', 'comment_enh', 'list_of_strings'\n",
    "           ]\n",
    "scripts.update_json_with_merged_df(df_sop_validated, columns, json_dir, sop_json_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Combine Subject-Predicate-Object\n",
    "- **Input:** Design description, subject-object pairs, and possible predicates.\n",
    "- **Output:** List of subject-predicate-object triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sop = pd.read_json(Path(json_dir) / sop_json_filename)\n",
    "df_sop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all entries in df_sop, for which validity is one value for each different value\n",
    "df_sop['validity_sop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sop_filtered = scripts.filter_sop_dataframe(df_sop, json_dir, pred_json_filename)\n",
    "df_sop_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_start = 0\n",
    "batch_stop = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_pred = prompts.find_predicates_prompts(df_sop_filtered, batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_pred, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_pred = scripts.process_prompts(prompts_pred, client, batch_start, batch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_pred = pd.DataFrame(responses_list_pred, index=None)\n",
    "df_pred_merged = df_sop_filtered.merge(df_responses_pred, \n",
    "                                                  on=['design_id','s_o_id'], how='left'\n",
    "                                                  )\n",
    "df_pred_merged.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Validate and Classify Extractes Relations\n",
    "- **Input:** List of subject-predicate-object triples.\n",
    "- **Output:** Validated and classified relations, marked as \"added predicates\" or \"used predicates in design\".\n",
    "\n",
    "#### Notes\n",
    "- Avoid/Filter predicates which a in the text, and a valid relation, but not in the design description.\n",
    "- Example 28/27\n",
    "    - Antoninus Pius\twearing\tWreath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_validate_pred = prompts.validate_spo_triples(df_pred_merged, batch_size)\n",
    "scripts.calculate_total_tokens_and_price(\n",
    "    prompts_validate_pred, batch_start, batch_stop, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list_validated_pred = scripts.process_prompts(prompts_validate_pred, client, batch_start, batch_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses_val_pred = pd.DataFrame(responses_list_validated_pred, index=None)\n",
    "print(df_responses_val_pred.head(5))\n",
    "df_pred_validated = df_responses_val_pred.merge(df_pred_merged, on=['design_id', 's_o_id'], how='left')\n",
    "\n",
    "df_pred_validated.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['design_id', 's_o_id', 's', 'subject_class', 'predicate', 'o', 'object_class', \n",
    "           \"validity_pred\", \"comment_pred\", \"implicit_pred\", \n",
    "           'validity_sop', 'comment_sop', 'design_en', 'new_list_of_strings', \n",
    "           'completeness', 'relevance', 'correctness', 'comment_enh', 'list_of_strings'\n",
    "           ]\n",
    "\n",
    "scripts.update_json_with_merged_df(df_pred_validated, columns, json_dir, pred_json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_validated = pd.read_json(Path(json_dir) / pred_json_filename)\n",
    "df_pred_validated.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_challenge_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
